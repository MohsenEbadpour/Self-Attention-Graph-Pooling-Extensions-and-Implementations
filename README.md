# Self-Attention Graph Pooling Extensions and Implementations

Welcome to the Self-Attention Graph Pooling Extensions and Implementations repository! This project extends and implements novel approaches based on the "Self-Attention Graph Pooling" paper, enhancing the original architecture with additional features.

## Overview

This repository showcases my final project for the Complex Networks (+Graph Neural Networks) course at Amirkabir University of Technology (AUT) during the Fall 2022 semester. As part of this project, I extended and implemented a **second approach** presented in the "Self-Attention Graph Pooling" paper, enriching the capabilities of graph neural networks.

## Key Features

- **Novel Approaches**: I have developed two novel approaches building upon the self-attention graph pooling method presented in the paper. These extensions offer unique perspectives on graph pooling and have the potential to improve various graph-related applications.

- **Learnable Weighting Mechanism**: A notable addition to this project is the incorporation of a learnable weighting mechanism. This mechanism dynamically adjusts weights during the concatenation step, enhancing the adaptability and performance of the graph pooling process.

- **Highest Project Score**: I am proud to share that this project achieved the highest score in the final project implementation category, emphasizing the quality and significance of the work.

## Tools and Frameworks

- **Programming Language**: Python
- **Deep Learning Framework**: PyTorch Geometric

Feel free to explore the code and project details in this repository. If you have any questions or suggestions, please don't hesitate to reach out.
